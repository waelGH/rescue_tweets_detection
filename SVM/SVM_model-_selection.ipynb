{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SVM Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Model selection code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wkhal001/envs/Twitter_crisis/lib/python3.7/site-packages/_distutils_hack/__init__.py:19: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n"
     ]
    }
   ],
   "source": [
    "##### imports ############\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/wkhal001/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#from utils_crisis_classification import clean_text\n",
    "from preprocessing_step import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wkhal001/envs/Twitter_crisis/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as pn\n",
    "from numpy import mean\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import string\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import statistics as s\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### imports ############\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#from utils_crisis_classification import clean_text\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from transformers import AdamW\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "from Baseline_Models import Display_metrics,Display_classification_report,Confusion_matrix\n",
    "from keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Harvey data set ##\n",
    "labeledDF=pd.read_csv(\"/home/wkhal001/Desktop/data_rescue_mining/labeled_ds_Corrected_csv_updated_22_09_13.csv\") \n",
    "del labeledDF['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'status_id', 'created_at', 'text', 'address', 'loc', 'situ',\n",
       "       'save', 'sos', 'sos.pred', 'sos.correct'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeledDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledDF = labeledDF[['id','created_at','text','sos.correct']]\n",
    "\n",
    "labeledDF['cleaned_tweet'] = labeledDF['text'].apply(lambda x: \" \".join(clean_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract useful columns \n",
    "df_training = labeledDF[['id','text','cleaned_tweet','sos.correct']]\n",
    "df_training.columns= ['id','non_cleaned_text','text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5520\n",
       "1     272\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SVM model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best SVM model on the training set using the repeated K-fold cross validation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF schemes: TF-IDF1 500 unigrams/bigrams -- TF-IDF2 500 unigrams -- TF-IDF3 1000 unigrams/bigrams -- TF-IDF4 1000 unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from Baseline_Models import Create_TFIDF,Vector_Encoding_TFIDF,Create_BOW,Vector_Encoding_BOW\n",
    "\n",
    "TFID = Create_TFIDF(df_training['text'],500,1,1)\n",
    "\n",
    "\n",
    "rep = 0\n",
    "save_sets = {}\n",
    "for i in range(5):\n",
    "    \n",
    "    #Train_Y.value_counts()\n",
    "    skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=rep)\n",
    "    \n",
    "    ###### SVM - vectorize input text using TF-IDF ######\n",
    "    Train_X_TFIDF = Vector_Encoding_TFIDF(TFID,df_training['text'])\n",
    "    \n",
    "    ###### defining parameter range ######\n",
    "    param_grid = {'C': [1,5,10,15,20,50,100,150],  \n",
    "              'gamma': [0.001,0.1,1,5,10],  \n",
    "              'degree' : [2,3],\n",
    "              'kernel': ['linear','poly','rbf']} \n",
    "   \n",
    "    ###### define grid search object ################\n",
    "    grid = GridSearchCV(SVC(),param_grid, cv = skf, refit = True, scoring = 'f1' ,verbose = 3,n_jobs=-1)   #cv = rkf   #scoring = 'f1'   #verbose = 3\n",
    "    \n",
    "    ###### fitting the model for grid search ################\n",
    "    grid.fit(Train_X_TFIDF, Train_Y)\n",
    "    \n",
    "    ###### save data and results for the current split ################\n",
    "    save_sets.update({rep:{'grid':grid}})\n",
    "    rep = rep + 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### save grids ##########\n",
    "ls_grid = [save_sets[0]['grid'],save_sets[1]['grid'],save_sets[2]['grid'],save_sets[3]['grid'],save_sets[4]['grid']]\n",
    "path = \"/home/wkhal001/Crisis-classification/scripts-rescue-detection/___rescue_identification_experiments/SVM_experiments/SVM_CV_IAN_IDA\"\n",
    "\n",
    "iteration =1\n",
    "for g in ls_grid:  \n",
    "    save_path = path + \"/\" + \"grid-CV-iter\"+ str(iteration)\n",
    "    joblib.dump(g, save_path)\n",
    "    iteration = iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### load grids ##########\n",
    "import joblib\n",
    "\n",
    "### save repetitions grids\n",
    "grid1 = joblib.load(\"/home/wkhal001/Crisis-classification/scripts-rescue-detection/___rescue_identification_experiments/SVM_experiments/SVM_CV/grid-CV-iter1\")\n",
    "grid2 = joblib.load(\"/home/wkhal001/Crisis-classification/scripts-rescue-detection/___rescue_identification_experiments/SVM_experiments/SVM_CV/grid-CV-iter2\")\n",
    "grid3 = joblib.load(\"/home/wkhal001/Crisis-classification/scripts-rescue-detection/___rescue_identification_experiments/SVM_experiments/SVM_CV/grid-CV-iter3\")\n",
    "grid4 = joblib.load(\"/home/wkhal001/Crisis-classification/scripts-rescue-detection/___rescue_identification_experiments/SVM_experiments/SVM_CV/grid-CV-iter4\")\n",
    "grid5 = joblib.load(\"/home/wkhal001/Crisis-classification/scripts-rescue-detection/___rescue_identification_experiments/SVM_experiments/SVM_CV/grid-CV-iter5\")\n",
    "\n",
    "ls_grid = [grid1,grid2,grid3,grid4,grid5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### best models and scores\n",
    "best_models = [g.best_params_ for g in ls_grid]\n",
    "best_f_scores = [g.best_score_ for g in ls_grid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'C': 20, 'degree': 2, 'gamma': 0.1, 'kernel': 'rbf'},\n",
       " {'C': 150, 'degree': 2, 'gamma': 0.1, 'kernel': 'rbf'},\n",
       " {'C': 100, 'degree': 2, 'gamma': 0.1, 'kernel': 'rbf'},\n",
       " {'C': 20, 'degree': 2, 'gamma': 0.1, 'kernel': 'rbf'},\n",
       " {'C': 15, 'degree': 2, 'gamma': 0.1, 'kernel': 'rbf'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## best models in 5 reps\n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8376376169749038,\n",
       " 0.833931786859323,\n",
       " 0.834361789513754,\n",
       " 0.8302264750947363,\n",
       " 0.8348285178176182]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## best scores in 5 reps\n",
    "best_f_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
