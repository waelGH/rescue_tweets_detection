{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script for CNN experiments on rescue detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports (1) ##\n",
    "import pandas as pd\n",
    "import numpy as pn\n",
    "from numpy import mean\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import string\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "### imports (2) ##\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "\n",
    "### imports (3) ##\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "### imports (4) ##\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#### functions to convert labels to numerical ####\n",
    "##################################################\n",
    "def class2Index(classList,class2index):\n",
    "    return [class2index[c] for c in classList]\n",
    "\n",
    "\n",
    "def train_classes(classes):\n",
    "    class2index = {}\n",
    "    index2class = {}\n",
    "    classCount = 0\n",
    "    for cl in np.unique(classes):\n",
    "        if cl not in class2index:\n",
    "            class2index[cl] = classCount\n",
    "            index2class[classCount] = cl\n",
    "            classCount += 1\n",
    "            \n",
    "    return class2index,index2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence - THE weather 99 Is good https://www.google.com/ I make a sandwitch :D\n",
      "Cleaned...\n",
      "New sentence -  weather 99 good make sandwitch\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "#### functions to clean the corpus ###############\n",
    "##################################################\n",
    "def remove_punc(text): \n",
    "    text = \"\".join([char for char in text if char not in string.punctuation ])\n",
    "    #text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+',text)\n",
    "    return text\n",
    "\n",
    "def remove_url(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return(text)\n",
    "\n",
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in STOPWORDS]\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text): \n",
    "    # lower case\n",
    "    text_lower = lower_case(text)\n",
    "    \n",
    "    # remove puntuation\n",
    "    text_punc = remove_punc(text_lower) \n",
    "    \n",
    "    #remove URLS\n",
    "    text_url = remove_url(text_punc)\n",
    "    \n",
    "    # tokenization\n",
    "    text_tokens = tokenization(text_url)\n",
    "    \n",
    "    # remove stop words\n",
    "    no_stop_tokens = remove_stopwords(text_tokens)\n",
    "    \n",
    "    return no_stop_tokens\n",
    "\n",
    "##### test the cleaning function on a test sentence ###############################################\n",
    "sentence = 'THE weather 99 Is good https://www.google.com/ I make a sandwitch :D'\n",
    "print('Original sentence -',sentence) \n",
    "sent = clean_text(sentence)\n",
    "print('Cleaned...')\n",
    "print('New sentence - ',' '.join(sent))\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: I love going to the beach at 999 Tunisia is the most beautiful country!! here we are..\n",
      "Clean corpus ..\n",
      "Cleaned corpus -- ['love', 'going', 'beach', '999', 'tunisia', 'beautiful', 'country']\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################\n",
    "#########  function to clean and tokenize a corpus (to create a vocab given a corpus)  #################\n",
    "###        input: corpus         output: clean tokens ##################################################\n",
    "########################################################################################################\n",
    "def clean_corpus(corpus):\n",
    "    # convert all to lower case \n",
    "    corpus_lower = lower_case(corpus)\n",
    "    \n",
    "    # remove punctuation\n",
    "    corpus_punc = remove_punc(corpus_lower) \n",
    "    # remove punctuation from each token\n",
    "    #table = str.maketrans('', '', string.punctuation)\n",
    "    #tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    #remove URLS\n",
    "    corpus_url = remove_url(corpus_punc)\n",
    "    \n",
    "    # tokenization\n",
    "    corpus_tokens = tokenization(corpus_url)\n",
    "    # split into tokens by white space\n",
    "    #tokens = corpus.split()\n",
    "    \n",
    "    # remove stop words\n",
    "    no_stop_tokens = remove_stopwords(corpus_tokens)\n",
    "    # filter out stop words\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    #tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # filter out short tokens\n",
    "    final_tokens = [word for word in no_stop_tokens if len(word) > 1]\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "########################################################################################################\n",
    "#########  function to clean and tokenize a document based on a given vocabulary   #####################\n",
    "###        input: document         output: clean document's tokens #####################################\n",
    "########################################################################################################\n",
    "def clean_document_vocab(doc, vocab):\n",
    "    # convert to lower case \n",
    "    doc_lower = lower_case(doc)\n",
    "    \n",
    "    # remove punctuation\n",
    "    doc_punc = remove_punc(doc_lower) \n",
    "\n",
    "    #remove URLS\n",
    "    doc_url = remove_url(doc_punc)\n",
    "    \n",
    "    # tokenization\n",
    "    doc_tokens = tokenization(doc_url)\n",
    "    \n",
    "    # remove stop words\n",
    "    doc_no_stop_tokens = remove_stopwords(doc_tokens)\n",
    "    \n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in doc_no_stop_tokens if w in vocab]\n",
    "    \n",
    "    #tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "##### test on a corpus #####\n",
    "ls = ['I love going to the beach at 999','Tunisia is the most beautiful country!!','here we are..']\n",
    "corpus = \" \".join(ls)\n",
    "print('Corpus:',corpus)\n",
    "print('Clean corpus ..')\n",
    "c_corpus = clean_corpus(corpus)\n",
    "print('Cleaned corpus --',c_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Harvey Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledDF=pd.read_csv(\"/home/wkhal001/Desktop/data_rescue_mining/labeled_ds_Corrected_csv_updated_22_09_13.csv\") \n",
    "del labeledDF['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>status_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>address</th>\n",
       "      <th>loc</th>\n",
       "      <th>situ</th>\n",
       "      <th>save</th>\n",
       "      <th>sos</th>\n",
       "      <th>sos.pred</th>\n",
       "      <th>sos.correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9.020000e+17</td>\n",
       "      <td>8/28/2017 11:19</td>\n",
       "      <td>#Harvey floods TV station #KHOU in #Houston. h...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9.020000e+17</td>\n",
       "      <td>8/28/2017 16:58</td>\n",
       "      <td>@RandiRhodes RR call him out for visiting SA, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9.020000e+17</td>\n",
       "      <td>8/27/2017 15:35</td>\n",
       "      <td>Wow a tv station is flooding in Houston! So sc...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9.020000e+17</td>\n",
       "      <td>8/29/2017 3:36</td>\n",
       "      <td>My son, dil &amp;amp; 2 grandkids in grand lakes, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>9.020000e+17</td>\n",
       "      <td>8/28/2017 12:09</td>\n",
       "      <td>is the beltway still flooded? ya boy need to g...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>5788</td>\n",
       "      <td>9.020000e+17</td>\n",
       "      <td>8/29/2017 4:04</td>\n",
       "      <td>Around 10,000,000,000,000 gallons of water fro...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>5789</td>\n",
       "      <td>9.020000e+17</td>\n",
       "      <td>8/29/2017 4:09</td>\n",
       "      <td>The road to my residence is flooded. Thank God...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>5790</td>\n",
       "      <td>9.020000e+17</td>\n",
       "      <td>8/29/2017 4:09</td>\n",
       "      <td>Texas road closures and flooding kept up to da...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>5791</td>\n",
       "      <td>9.020000e+17</td>\n",
       "      <td>8/29/2017 4:22</td>\n",
       "      <td>@HellerWeather Tim, any maps to show where flo...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5791</th>\n",
       "      <td>5792</td>\n",
       "      <td>9.020000e+17</td>\n",
       "      <td>8/29/2017 4:25</td>\n",
       "      <td>lrt more people died on the road trying to eva...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5792 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     status_id       created_at  \\\n",
       "0        1  9.020000e+17  8/28/2017 11:19   \n",
       "1        2  9.020000e+17  8/28/2017 16:58   \n",
       "2        3  9.020000e+17  8/27/2017 15:35   \n",
       "3        4  9.020000e+17   8/29/2017 3:36   \n",
       "4        5  9.020000e+17  8/28/2017 12:09   \n",
       "...    ...           ...              ...   \n",
       "5787  5788  9.020000e+17   8/29/2017 4:04   \n",
       "5788  5789  9.020000e+17   8/29/2017 4:09   \n",
       "5789  5790  9.020000e+17   8/29/2017 4:09   \n",
       "5790  5791  9.020000e+17   8/29/2017 4:22   \n",
       "5791  5792  9.020000e+17   8/29/2017 4:25   \n",
       "\n",
       "                                                   text  address  loc  situ  \\\n",
       "0     #Harvey floods TV station #KHOU in #Houston. h...        0  NaN     0   \n",
       "1     @RandiRhodes RR call him out for visiting SA, ...        0  NaN     0   \n",
       "2     Wow a tv station is flooding in Houston! So sc...        0  NaN     1   \n",
       "3     My son, dil &amp; 2 grandkids in grand lakes, ...        0  NaN     0   \n",
       "4     is the beltway still flooded? ya boy need to g...        0  NaN     0   \n",
       "...                                                 ...      ...  ...   ...   \n",
       "5787  Around 10,000,000,000,000 gallons of water fro...        0  NaN     0   \n",
       "5788  The road to my residence is flooded. Thank God...        0  NaN     0   \n",
       "5789  Texas road closures and flooding kept up to da...        0  NaN     0   \n",
       "5790  @HellerWeather Tim, any maps to show where flo...        1  NaN     0   \n",
       "5791  lrt more people died on the road trying to eva...        0  NaN     0   \n",
       "\n",
       "      save  sos  sos.pred  sos.correct  \n",
       "0        0    0         0            0  \n",
       "1        0    0         0            0  \n",
       "2        0    0         0            0  \n",
       "3        1    1         1            0  \n",
       "4        0    0         0            0  \n",
       "...    ...  ...       ...          ...  \n",
       "5787     0    0         0            0  \n",
       "5788     0    0         0            0  \n",
       "5789     0    0         0            0  \n",
       "5790     0    0         0            0  \n",
       "5791     0    0         0            0  \n",
       "\n",
       "[5792 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeledDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean our data set\n",
    "labeledDF['cleaned_tweet'] = labeledDF['text'].apply(lambda x: \" \".join(clean_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract useful columns \n",
    "df_training = labeledDF[['id','text','cleaned_tweet','sos.correct']]\n",
    "df_training.columns= ['id','non_cleaned_text','text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>non_cleaned_text</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>#Harvey floods TV station #KHOU in #Houston. h...</td>\n",
       "      <td>harvey floods tv station khou houston</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@RandiRhodes RR call him out for visiting SA, ...</td>\n",
       "      <td>randirhodes rr call visiting sa flooding mayor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Wow a tv station is flooding in Houston! So sc...</td>\n",
       "      <td>wow tv station flooding houston scary sad rain...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>My son, dil &amp;amp; 2 grandkids in grand lakes, ...</td>\n",
       "      <td>son dil amp 2 grandkids grand lakes katy tx wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>is the beltway still flooded? ya boy need to g...</td>\n",
       "      <td>beltway still flooded ya boy need go pay bills</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>5788</td>\n",
       "      <td>Around 10,000,000,000,000 gallons of water fro...</td>\n",
       "      <td>around 10000000000000 gallons water harvey ins...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>5789</td>\n",
       "      <td>The road to my residence is flooded. Thank God...</td>\n",
       "      <td>road residence flooded thank god left safe sta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>5790</td>\n",
       "      <td>Texas road closures and flooding kept up to da...</td>\n",
       "      <td>texas road closures flooding kept date</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>5791</td>\n",
       "      <td>@HellerWeather Tim, any maps to show where flo...</td>\n",
       "      <td>hellerweather tim maps show flooding would mas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5791</th>\n",
       "      <td>5792</td>\n",
       "      <td>lrt more people died on the road trying to eva...</td>\n",
       "      <td>lrt people died road trying evacuate hurricane...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5792 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                   non_cleaned_text  \\\n",
       "0        1  #Harvey floods TV station #KHOU in #Houston. h...   \n",
       "1        2  @RandiRhodes RR call him out for visiting SA, ...   \n",
       "2        3  Wow a tv station is flooding in Houston! So sc...   \n",
       "3        4  My son, dil &amp; 2 grandkids in grand lakes, ...   \n",
       "4        5  is the beltway still flooded? ya boy need to g...   \n",
       "...    ...                                                ...   \n",
       "5787  5788  Around 10,000,000,000,000 gallons of water fro...   \n",
       "5788  5789  The road to my residence is flooded. Thank God...   \n",
       "5789  5790  Texas road closures and flooding kept up to da...   \n",
       "5790  5791  @HellerWeather Tim, any maps to show where flo...   \n",
       "5791  5792  lrt more people died on the road trying to eva...   \n",
       "\n",
       "                                                   text  label  \n",
       "0                harvey floods tv station khou houston       0  \n",
       "1     randirhodes rr call visiting sa flooding mayor...      0  \n",
       "2     wow tv station flooding houston scary sad rain...      0  \n",
       "3     son dil amp 2 grandkids grand lakes katy tx wo...      0  \n",
       "4       beltway still flooded ya boy need go pay bills       0  \n",
       "...                                                 ...    ...  \n",
       "5787  around 10000000000000 gallons water harvey ins...      0  \n",
       "5788  road residence flooded thank god left safe sta...      0  \n",
       "5789            texas road closures flooding kept date       0  \n",
       "5790  hellerweather tim maps show flooding would mas...      0  \n",
       "5791  lrt people died road trying evacuate hurricane...      0  \n",
       "\n",
       "[5792 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5520\n",
       "1     272\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Ian/Ida data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#### read mixed data - verified ####################\n",
    "####################################################\n",
    "path_to_mixed_verif = '/home/wkhal001/Desktop/Mixed_data_verified/Mixed_data__ian_ida_verified.csv'\n",
    "\n",
    "mixed_ida_ian_verified=pd.read_csv(path_to_mixed_verif) \n",
    "del mixed_ida_ian_verified['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4935\n",
       "1     225\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_ida_ian_verified['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_mixed = mixed_ida_ian_verified[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_mixed['cleaned_tweet'] = Data_mixed['text'].apply(lambda x: \" \".join(clean_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract useful columns \n",
    "df_training = Data_mixed[['text','cleaned_tweet','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.columns= ['non_cleaned_text','text','label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vocabulary and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created... 11722  tokens\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "# Create a vocabulary   (specify the corpus data frame (e.g., labeledDF) #########\n",
    "##################################################################################\n",
    "corpus = ''\n",
    "for i in range(len(df_training)) :\n",
    "    st = df_training.iloc[i][\"non_cleaned_text\"]\n",
    "    corpus = corpus + \" \" + st\n",
    "    \n",
    "\n",
    "#corpus\n",
    "T = clean_corpus(corpus)\n",
    "\n",
    "### count vocabulary with collection counter ####\n",
    "vocab = Counter()\n",
    "vocab.update(T)\n",
    "\n",
    "print('Vocabulary created...',len(vocab),' tokens')\n",
    "\n",
    "# with open('vocab_crisis_bench.pickle', 'wb') as outputfile:\n",
    "#     pickle.dump(vocab, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload Pretrained word embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193515 word vectors.\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "########      Load pretrained embedding    #######################################\n",
    "##################################################################################\n",
    "path_to_glove = '/home/wkhal001/Desktop/gensim-data/glove-twitter-200/glove-twitter-200.txt' \n",
    "    \n",
    "# Load the file content in a dictionary\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "#embeddings_index['hurricane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size ... 11749\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "###### create Keras tokenizer and fit text ############\n",
    "#######################################################\n",
    "tokenizer = Tokenizer(num_words=None,oov_token='OOV')\n",
    "tokenizer.fit_on_texts(df_training['text'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Tokenizer vocabulary size ...',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocaublary size: 11749\n",
      "Embedding dimension: 200\n",
      "Create embedding matrix in progress....\n",
      "Embedding matrix created\n",
      "Embedding matrix size 11749\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "########      Initialize the embedding layer    ##################################\n",
    "##################################################################################\n",
    "EMBEDDING_DIM=200\n",
    "vocabulary_size=len(tokenizer.word_index)+1\n",
    "print('Vocaublary size:',vocabulary_size)\n",
    "print('Embedding dimension:',EMBEDDING_DIM)\n",
    "print('Create embedding matrix in progress....')\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Embedding matrix created')\n",
    "print('Embedding matrix size',len(embedding_matrix))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Define encoder architecture Kim model  #################################\n",
    "##########################################################################\n",
    "def create_kim_encoder(length, vocab_size=100, embedding=False, embed_params={}):\n",
    "    ## inputs #####\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    \n",
    "    ## Create the embedding layer  ######################################\n",
    "    if embedding == True:   \n",
    "        embedding_layer = Embedding(embed_params['Tokenizer_size'] + 1,\n",
    "                            embed_params['EMBEDDING_DIM'],\n",
    "                            weights=[embed_params['weights']],\n",
    "                            input_length=embed_params['input_length'],\n",
    "                            trainable=False)\n",
    "        embedding1 = embedding_layer(inputs1)\n",
    "    else: \n",
    "        embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    \n",
    "\n",
    "    # channel 1\n",
    "    #embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    embedding1 = embedding_layer(inputs1)\n",
    "    conv1 = Conv1D(filters=1024, kernel_size=2, activation='relu')(embedding1)\n",
    "    #drop1 = Dropout(0.3)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=8)(conv1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "\n",
    "    # channel 2\n",
    "    #inputs2 = Input(shape=(length,))\n",
    "    #embedding2 = Embedding(vocab_size, 100)(inputs1)\n",
    "    embedding2 = embedding_layer(inputs1)\n",
    "    conv2 = Conv1D(filters=128, kernel_size=16, activation='relu')(embedding2)\n",
    "    #drop2 = Dropout(0.3)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=6)(conv2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "\n",
    "    # channel 3\n",
    "    #inputs3 = Input(shape=(length,))\n",
    "    #embedding3 = Embedding(vocab_size, 100)(inputs1)\n",
    "    embedding3 = embedding_layer(inputs1)\n",
    "    conv3 = Conv1D(filters=16, kernel_size=14, activation='relu')(embedding3)\n",
    "    #drop3 = Dropout(0.3)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=6)(conv3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    \n",
    "    # channel 4\n",
    "    #inputs3 = Input(shape=(length,))\n",
    "    #embedding4 = Embedding(vocab_size, 100)(inputs1)\n",
    "    # embedding4 = embedding_layer(inputs1)\n",
    "    # conv4 = Conv1D(filters=700, kernel_size=2, activation='relu')(embedding3)\n",
    "    # #drop4 = Dropout(0.5)(conv4)\n",
    "    # pool4 = MaxPooling1D(pool_size=2)(conv4)\n",
    "    # flat4 = Flatten()(pool4)\n",
    "\n",
    "    # merge\n",
    "    union = concatenate([flat1, flat2, flat3])\n",
    "    #union = union.reshape(union.size(0), -1)\n",
    "    \n",
    "    # interpretation\n",
    "    #dense1 = Dense(300, activation='relu')(merged)\n",
    "    #outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = keras.Model(inputs=inputs1, outputs=union) #[inputs1, inputs2, inputs3]\n",
    "\n",
    "    # summarize\n",
    "    #print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='MMD_kim_encoder_kernel_3_4_8.png')\n",
    "  \n",
    "    return model\n",
    "\n",
    "##########################################################################\n",
    "# Define encoder architecture single channel  ############################\n",
    "##########################################################################\n",
    "def create_single_channel_cnn_encoder(length, vocab_size=100):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "\n",
    "    # channel 1\n",
    "    #embedding1 = embedding_layer(vocab_size, 100)(inputs1)\n",
    "    embedding1 = embedding_layer(inputs1)\n",
    "    conv1 = Conv1D(filters=128, kernel_size=8, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    \n",
    "    flat1 = Flatten()(pool1)\n",
    "    model = keras.Model(inputs=inputs1, outputs=flat1) #[inputs1, inputs2, inputs3] outputs=union\n",
    "\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='MMD_single_channel_encoder_kernel_8.png')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "###  Create a classifier on top of the CNN architectures #################\n",
    "##########################################################################\n",
    "def create_classifier(encoder, trainable=True):\n",
    "    num_classes=2\n",
    "    input_shape = (128,)\n",
    "    hidden_units = 64\n",
    "    \n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "        \n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(0.3)(features)\n",
    "    features = layers.Dense(64, activation=\"relu\")(features)\n",
    "    #features = layers.Dropout(dropout_rate)(features)\n",
    "    #features = layers.Dense(50, activation=\"relu\")(features)\n",
    "    #features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "    \n",
    "    # inputs = keras.Input(shape=input_shape)\n",
    "    # features = encoder(inputs)\n",
    "    # features = layers.Dropout(dropout_rate)(features)\n",
    "    # features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    # features = layers.Dropout(dropout_rate)(features)\n",
    "    # outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cnn-glove-classifier-harvey\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from transformers import AdamW\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from Baseline_Models import Display_metrics,Display_classification_report,Confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")\n",
    "device\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=2018, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=2018)\n",
    "\n",
    "X = df_training['text']\n",
    "TX = np.array(X.tolist())\n",
    "\n",
    "Y = df_training['label']\n",
    "TY= np.array(Y.tolist())\n",
    "\n",
    "print(skf)\n",
    "\n",
    "for train_index, test_index in skf.split(TX,TY):\n",
    "    print('--------- Fold ',str(rep_fold),'-------------------------')\n",
    "    print('Length train index....',len(train_index))\n",
    "    print('Length test index....',len(test_index))\n",
    "    \n",
    "    #### training/testing data ########################################\n",
    "    X_train, X_test = TX[train_index], TX[test_index]\n",
    "    y_train, y_test = TY[train_index], TY[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Fold  1 -------------------------\n",
      "Length train index.... 4644\n",
      "Length test index.... 516\n",
      "postive samples in train...... 182\n",
      "postive samples in validation...... 20\n",
      "postive samples in test...... 23\n",
      "------------------------------------------\n",
      "cleaned train docs (in tokens) loaded....\n",
      "train max.... 77\n",
      "cleaned validation docs (in tokens) loaded....\n",
      "train max.... 44\n",
      "cleaned test docs (in tokens) loaded....\n",
      "test max.... 33\n",
      "Model: \"cnn-glove-classifier-harvey\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " model_4 (Functional)        (None, 17968)             3214968   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 17968)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                1150016   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,365,114\n",
      "Trainable params: 4,365,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 7s 179ms/step - loss: 0.1896 - sparse_categorical_accuracy: 0.9457 - val_loss: 0.1177 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0727 - sparse_categorical_accuracy: 0.9564 - val_loss: 0.0722 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0436 - sparse_categorical_accuracy: 0.9782 - val_loss: 0.0700 - val_sparse_categorical_accuracy: 0.9720\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0358 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.0694 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0331 - sparse_categorical_accuracy: 0.9938 - val_loss: 0.0741 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.0312 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.0757 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.0301 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0878 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0295 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0781 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0289 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0801 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 5s 167ms/step - loss: 0.0283 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0834 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 6s 167ms/step - loss: 0.0278 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0838 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0836 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 6s 167ms/step - loss: 0.0268 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0829 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 5s 167ms/step - loss: 0.0263 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0865 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 6s 167ms/step - loss: 0.0259 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0832 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.0255 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0830 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.0250 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0829 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 5s 167ms/step - loss: 0.0246 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0839 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0882 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0238 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0811 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0830 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.0229 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0896 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 6s 167ms/step - loss: 0.0225 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0837 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0222 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0871 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0896 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0846 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0210 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0826 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0867 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0830 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0199 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0819 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0791 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0860 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0189 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0908 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0807 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0810 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0851 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0826 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0855 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 5s 167ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0848 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0168 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0908 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0165 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0874 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 6s 167ms/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0897 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0818 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0157 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0829 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0124 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0723 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0885 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0744 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.1052 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0020 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0872 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0012 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0763 - val_sparse_categorical_accuracy: 0.9785\n",
      "calculate preds....\n",
      "AP score for class 1 ---> 0.8796721034647297\n",
      "f1-score...... 0.7727272727272727\n",
      "recall...... 0.7391304347826086\n",
      "recall...... 0.8095238095238095\n",
      "------------------------------------------------------\n",
      "--------- Fold  2 -------------------------\n",
      "Length train index.... 4644\n",
      "Length test index.... 516\n",
      "postive samples in train...... 182\n",
      "postive samples in validation...... 20\n",
      "postive samples in test...... 23\n",
      "------------------------------------------\n",
      "cleaned train docs (in tokens) loaded....\n",
      "train max.... 77\n",
      "cleaned validation docs (in tokens) loaded....\n",
      "train max.... 44\n",
      "cleaned test docs (in tokens) loaded....\n",
      "test max.... 36\n",
      "Model: \"cnn-glove-classifier-harvey\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " model_5 (Functional)        (None, 17968)             3214968   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 17968)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                1150016   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,365,114\n",
      "Trainable params: 4,365,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 7s 181ms/step - loss: 0.2259 - sparse_categorical_accuracy: 0.9387 - val_loss: 0.1282 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0830 - sparse_categorical_accuracy: 0.9564 - val_loss: 0.0729 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0466 - sparse_categorical_accuracy: 0.9718 - val_loss: 0.0662 - val_sparse_categorical_accuracy: 0.9720\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0370 - sparse_categorical_accuracy: 0.9842 - val_loss: 0.0678 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0337 - sparse_categorical_accuracy: 0.9904 - val_loss: 0.0668 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0322 - sparse_categorical_accuracy: 0.9947 - val_loss: 0.0713 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0309 - sparse_categorical_accuracy: 0.9974 - val_loss: 0.0773 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0301 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0821 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0295 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0751 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0289 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0844 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0284 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0809 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0279 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0911 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0276 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0797 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0271 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0818 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0266 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0905 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0262 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0843 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0258 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0912 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0254 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0829 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0249 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0898 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0873 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0241 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0883 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0857 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0234 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0863 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0802 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0891 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0223 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0742 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0865 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0216 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0767 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0212 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0891 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0208 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0920 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0205 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0881 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0881 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0808 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0873 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0787 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0189 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0883 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0850 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0870 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0760 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0832 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0825 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0856 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0169 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0862 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0166 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0847 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0836 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0161 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0812 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0875 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0157 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0716 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0944 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0151 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0839 - val_sparse_categorical_accuracy: 0.9806\n",
      "calculate preds....\n",
      "AP score for class 1 ---> 0.8083145709623994\n",
      "f1-score...... 0.75\n",
      "recall...... 0.6521739130434783\n",
      "recall...... 0.8823529411764706\n",
      "------------------------------------------------------\n",
      "--------- Fold  3 -------------------------\n",
      "Length train index.... 4644\n",
      "Length test index.... 516\n",
      "postive samples in train...... 182\n",
      "postive samples in validation...... 20\n",
      "postive samples in test...... 23\n",
      "------------------------------------------\n",
      "cleaned train docs (in tokens) loaded....\n",
      "train max.... 77\n",
      "cleaned validation docs (in tokens) loaded....\n",
      "train max.... 34\n",
      "cleaned test docs (in tokens) loaded....\n",
      "test max.... 44\n",
      "Model: \"cnn-glove-classifier-harvey\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " model_6 (Functional)        (None, 17968)             3214968   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 17968)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                1150016   \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,365,114\n",
      "Trainable params: 4,365,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 7s 179ms/step - loss: 0.2130 - sparse_categorical_accuracy: 0.9287 - val_loss: 0.1239 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0865 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.0741 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0474 - sparse_categorical_accuracy: 0.9746 - val_loss: 0.0622 - val_sparse_categorical_accuracy: 0.9720\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0299 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.0594 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0646 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0065 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0620 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0043 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0663 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0024 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0693 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0018 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0820 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0018 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0847 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0018 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0776 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0823 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0016 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0954 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0018 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0716 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0011 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0957 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.1028 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0020 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0735 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0016 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1213 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0785 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0014 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0818 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0033 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0970 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0027 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0709 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0015 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0894 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0741 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0011 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0802 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 6.8115e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0991 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0010 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0808 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0012 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0918 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 8.0730e-04 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1089 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0017 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0867 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 7.2125e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.1054 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0018 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0915 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0012 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0917 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 7.3060e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0860 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1124 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 8.7517e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.1028 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 6.9544e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.1009 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 7.0225e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0957 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 8.7544e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.1074 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0018 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.1010 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0011 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.1021 - val_sparse_categorical_accuracy: 0.9828\n",
      "calculate preds....\n",
      "AP score for class 1 ---> 0.9655461790598037\n",
      "f1-score...... 0.8837209302325583\n",
      "recall...... 0.8260869565217391\n",
      "recall...... 0.95\n",
      "------------------------------------------------------\n",
      "--------- Fold  4 -------------------------\n",
      "Length train index.... 4644\n",
      "Length test index.... 516\n",
      "postive samples in train...... 182\n",
      "postive samples in validation...... 20\n",
      "postive samples in test...... 23\n",
      "------------------------------------------\n",
      "cleaned train docs (in tokens) loaded....\n",
      "train max.... 77\n",
      "cleaned validation docs (in tokens) loaded....\n",
      "train max.... 44\n",
      "cleaned test docs (in tokens) loaded....\n",
      "test max.... 34\n",
      "Model: \"cnn-glove-classifier-harvey\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " model_7 (Functional)        (None, 17968)             3214968   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 17968)             0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                1150016   \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,365,114\n",
      "Trainable params: 4,365,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 7s 179ms/step - loss: 0.1983 - sparse_categorical_accuracy: 0.9497 - val_loss: 0.1165 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0790 - sparse_categorical_accuracy: 0.9564 - val_loss: 0.0671 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0429 - sparse_categorical_accuracy: 0.9773 - val_loss: 0.0587 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0355 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.0622 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0327 - sparse_categorical_accuracy: 0.9909 - val_loss: 0.0615 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0317 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.0641 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0308 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.0632 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0302 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0636 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0296 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.0654 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0291 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0652 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0287 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0631 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0282 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0680 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0277 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0685 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0674 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0268 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0693 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0264 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0668 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0260 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0656 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0256 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0695 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0643 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0657 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0244 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0698 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0593 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0665 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0647 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0229 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0662 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0225 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0657 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0221 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0667 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0628 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0215 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0652 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0211 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0612 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0208 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0633 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0624 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0667 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0607 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0648 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0191 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0613 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0189 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0639 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0646 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0641 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0603 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0628 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0175 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0639 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0662 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0170 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0674 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0167 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0658 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0663 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0162 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0668 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0675 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0650 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0156 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0560 - val_sparse_categorical_accuracy: 0.9892\n",
      "calculate preds....\n",
      "AP score for class 1 ---> 0.9072363123731497\n",
      "f1-score...... 0.85\n",
      "recall...... 0.7391304347826086\n",
      "recall...... 1.0\n",
      "------------------------------------------------------\n",
      "--------- Fold  5 -------------------------\n",
      "Length train index.... 4644\n",
      "Length test index.... 516\n",
      "postive samples in train...... 182\n",
      "postive samples in validation...... 20\n",
      "postive samples in test...... 23\n",
      "------------------------------------------\n",
      "cleaned train docs (in tokens) loaded....\n",
      "train max.... 77\n",
      "cleaned validation docs (in tokens) loaded....\n",
      "train max.... 36\n",
      "cleaned test docs (in tokens) loaded....\n",
      "test max.... 35\n",
      "Model: \"cnn-glove-classifier-harvey\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " model_8 (Functional)        (None, 17968)             3214968   \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 17968)             0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                1150016   \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,365,114\n",
      "Trainable params: 4,365,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 7s 187ms/step - loss: 0.2011 - sparse_categorical_accuracy: 0.9474 - val_loss: 0.1244 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0797 - sparse_categorical_accuracy: 0.9564 - val_loss: 0.0744 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 6s 180ms/step - loss: 0.0456 - sparse_categorical_accuracy: 0.9730 - val_loss: 0.0706 - val_sparse_categorical_accuracy: 0.9677\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0366 - sparse_categorical_accuracy: 0.9883 - val_loss: 0.0819 - val_sparse_categorical_accuracy: 0.9656\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9911 - val_loss: 0.0859 - val_sparse_categorical_accuracy: 0.9656\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0314 - sparse_categorical_accuracy: 0.9952 - val_loss: 0.0866 - val_sparse_categorical_accuracy: 0.9677\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0301 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0932 - val_sparse_categorical_accuracy: 0.9656\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0294 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0872 - val_sparse_categorical_accuracy: 0.9720\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0289 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0830 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0283 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0906 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 6s 180ms/step - loss: 0.0278 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0880 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0274 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0965 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 6s 180ms/step - loss: 0.0269 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0936 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 6s 180ms/step - loss: 0.0264 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0952 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 6s 179ms/step - loss: 0.0260 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0912 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0256 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0999 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0251 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0930 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0247 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0884 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 6s 179ms/step - loss: 0.0243 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1024 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 6s 179ms/step - loss: 0.0238 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1026 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 6s 180ms/step - loss: 0.0234 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0978 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0930 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1011 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0222 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0974 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 6s 178ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0982 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0215 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1034 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0211 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1071 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0207 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0987 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0975 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0200 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0927 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0197 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0991 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0901 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1013 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0868 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0993 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0940 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0956 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0969 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0944 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0168 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0981 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0165 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0986 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0162 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0997 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0908 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0157 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0974 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0881 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0152 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0912 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0149 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0988 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0147 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0976 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0144 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0958 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0142 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0970 - val_sparse_categorical_accuracy: 0.9806\n",
      "calculate preds....\n",
      "AP score for class 1 ---> 0.9174809639395934\n",
      "f1-score...... 0.7999999999999999\n",
      "recall...... 0.6956521739130435\n",
      "recall...... 0.9411764705882353\n",
      "------------------------------------------------------\n",
      "--------- Fold  6 -------------------------\n",
      "Length train index.... 4644\n",
      "Length test index.... 516\n",
      "postive samples in train...... 183\n",
      "postive samples in validation...... 20\n",
      "postive samples in test...... 22\n",
      "------------------------------------------\n",
      "cleaned train docs (in tokens) loaded....\n",
      "train max.... 77\n",
      "cleaned validation docs (in tokens) loaded....\n",
      "train max.... 44\n",
      "cleaned test docs (in tokens) loaded....\n",
      "test max.... 36\n",
      "Model: \"cnn-glove-classifier-harvey\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " model_9 (Functional)        (None, 17968)             3214968   \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 17968)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                1150016   \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,365,114\n",
      "Trainable params: 4,365,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 7s 178ms/step - loss: 0.2269 - sparse_categorical_accuracy: 0.9526 - val_loss: 0.1219 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0837 - sparse_categorical_accuracy: 0.9562 - val_loss: 0.0730 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0468 - sparse_categorical_accuracy: 0.9703 - val_loss: 0.0641 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0372 - sparse_categorical_accuracy: 0.9852 - val_loss: 0.0660 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0342 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0692 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0321 - sparse_categorical_accuracy: 0.9947 - val_loss: 0.0727 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0311 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.0741 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0305 - sparse_categorical_accuracy: 0.9966 - val_loss: 0.0758 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0297 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0776 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0293 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0762 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0288 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0822 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0281 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0869 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0272 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0895 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0266 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0880 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0262 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0835 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0257 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0857 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0967 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0891 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0244 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0893 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0239 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0958 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0236 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0874 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0231 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0916 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0227 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0778 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0224 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0881 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0821 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0215 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0908 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0210 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0956 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0208 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0852 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0886 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0200 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0835 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0878 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0851 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0189 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0868 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0796 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0824 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0179 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0857 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0813 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0839 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0170 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0877 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0166 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0871 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0913 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0913 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0157 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0869 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0155 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0839 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0153 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0725 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0150 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0798 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0147 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0847 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0145 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0852 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0141 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0862 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0140 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0789 - val_sparse_categorical_accuracy: 0.9806\n",
      "calculate preds....\n",
      "AP score for class 1 ---> 0.9941460055096418\n",
      "f1-score...... 0.7777777777777778\n",
      "recall...... 0.6363636363636364\n",
      "recall...... 1.0\n",
      "------------------------------------------------------\n",
      "--------- Fold  7 -------------------------\n",
      "Length train index.... 4644\n",
      "Length test index.... 516\n",
      "postive samples in train...... 183\n",
      "postive samples in validation...... 20\n",
      "postive samples in test...... 22\n",
      "------------------------------------------\n",
      "cleaned train docs (in tokens) loaded....\n",
      "train max.... 44\n",
      "cleaned validation docs (in tokens) loaded....\n",
      "train max.... 65\n",
      "cleaned test docs (in tokens) loaded....\n",
      "test max.... 77\n",
      "Model: \"cnn-glove-classifier-harvey\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " model_10 (Functional)       (None, 17968)             3214968   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 17968)             0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 64)                1150016   \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,365,114\n",
      "Trainable params: 4,365,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 7s 179ms/step - loss: 0.2299 - sparse_categorical_accuracy: 0.9344 - val_loss: 0.1401 - val_sparse_categorical_accuracy: 0.9548\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0964 - sparse_categorical_accuracy: 0.9562 - val_loss: 0.0781 - val_sparse_categorical_accuracy: 0.9548\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0516 - sparse_categorical_accuracy: 0.9639 - val_loss: 0.0671 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0389 - sparse_categorical_accuracy: 0.9840 - val_loss: 0.0683 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0352 - sparse_categorical_accuracy: 0.9911 - val_loss: 0.0699 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0335 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.0698 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0321 - sparse_categorical_accuracy: 0.9966 - val_loss: 0.0696 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0312 - sparse_categorical_accuracy: 0.9974 - val_loss: 0.0792 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0302 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0801 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0295 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0817 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0288 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0824 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0282 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0822 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0278 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0775 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0880 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0268 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0827 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0263 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0857 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0259 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0722 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0254 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0855 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0249 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0841 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0246 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0827 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0811 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0816 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0845 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0229 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0836 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0224 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0828 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0221 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0802 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0216 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0886 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0213 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0811 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0209 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0822 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0205 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0808 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0207 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0727 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0809 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0820 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0814 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0189 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0828 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0840 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0880 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0179 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0817 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0882 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0835 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0170 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0851 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0167 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0772 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0781 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0785 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0799 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0157 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0802 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0153 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0718 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0150 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0819 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0147 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0839 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0145 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0816 - val_sparse_categorical_accuracy: 0.9828\n",
      "calculate preds....\n",
      "AP score for class 1 ---> 0.9109465891185135\n",
      "f1-score...... 0.7222222222222223\n",
      "recall...... 0.5909090909090909\n",
      "recall...... 0.9285714285714286\n",
      "------------------------------------------------------\n",
      "--------- Fold  8 -------------------------\n",
      "Length train index.... 4644\n",
      "Length test index.... 516\n",
      "postive samples in train...... 183\n",
      "postive samples in validation...... 20\n",
      "postive samples in test...... 22\n",
      "------------------------------------------\n",
      "cleaned train docs (in tokens) loaded....\n",
      "train max.... 77\n",
      "cleaned validation docs (in tokens) loaded....\n",
      "train max.... 36\n",
      "cleaned test docs (in tokens) loaded....\n",
      "test max.... 65\n",
      "Model: \"cnn-glove-classifier-harvey\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_24 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " model_11 (Functional)       (None, 17968)             3214968   \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 17968)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                1150016   \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,365,114\n",
      "Trainable params: 4,365,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 7s 182ms/step - loss: 0.2025 - sparse_categorical_accuracy: 0.9414 - val_loss: 0.1146 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0814 - sparse_categorical_accuracy: 0.9562 - val_loss: 0.0635 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0469 - sparse_categorical_accuracy: 0.9727 - val_loss: 0.0598 - val_sparse_categorical_accuracy: 0.9699\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9864 - val_loss: 0.0598 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0333 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0661 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0313 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.0651 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0306 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.0697 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0298 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0653 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0294 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.0646 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0288 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0679 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0281 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0695 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0277 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0731 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0271 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0745 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0626 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0264 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0693 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0259 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0707 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0254 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0676 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0250 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0679 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0246 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0743 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0241 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0739 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0751 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0725 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0229 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0679 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0224 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0755 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0222 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0732 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0727 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0213 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0708 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0210 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0615 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0207 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0666 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0644 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0200 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0582 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0699 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0625 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0694 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0728 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0749 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0178 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0738 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 6s 176ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0686 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0672 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0170 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0621 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0167 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0679 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0165 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0741 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0152 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0579 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0024 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0697 - val_sparse_categorical_accuracy: 0.9806\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0617 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0024 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0477 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 0.0064 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0569 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0692 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0087 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.1114 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0041 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0769 - val_sparse_categorical_accuracy: 0.9849\n",
      "calculate preds....\n",
      "AP score for class 1 ---> 0.9095587586677312\n",
      "f1-score...... 0.8095238095238095\n",
      "recall...... 0.7727272727272727\n",
      "recall...... 0.85\n",
      "------------------------------------------------------\n",
      "--------- Fold  9 -------------------------\n",
      "Length train index.... 4644\n",
      "Length test index.... 516\n",
      "postive samples in train...... 183\n",
      "postive samples in validation...... 20\n",
      "postive samples in test...... 22\n",
      "------------------------------------------\n",
      "cleaned train docs (in tokens) loaded....\n",
      "train max.... 77\n",
      "cleaned validation docs (in tokens) loaded....\n",
      "train max.... 36\n",
      "cleaned test docs (in tokens) loaded....\n",
      "test max.... 43\n",
      "Model: \"cnn-glove-classifier-harvey\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_26 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " model_12 (Functional)       (None, 17968)             3214968   \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 17968)             0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 64)                1150016   \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,365,114\n",
      "Trainable params: 4,365,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 7s 179ms/step - loss: 0.1969 - sparse_categorical_accuracy: 0.9550 - val_loss: 0.1136 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0752 - sparse_categorical_accuracy: 0.9562 - val_loss: 0.0566 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0446 - sparse_categorical_accuracy: 0.9782 - val_loss: 0.0465 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0363 - sparse_categorical_accuracy: 0.9854 - val_loss: 0.0519 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9938 - val_loss: 0.0538 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0353 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0054 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0391 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0033 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0252 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 0.0025 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0353 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0266 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0036 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0457 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0017 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0295 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0016 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0648 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0668 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0012 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0366 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0619 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0017 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0308 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 9.4235e-04 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0387 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 5.0668e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0374 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 6s 173ms/step - loss: 0.0015 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0244 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0824 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 6s 174ms/step - loss: 8.8718e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0264 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 4.2486e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0338 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0012 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0624 - val_sparse_categorical_accuracy: 0.9828\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0284 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0013 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0564 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 7.3601e-04 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0505 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 5.6692e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0385 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 9.2529e-04 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0604 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0015 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0235 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 7.8215e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0585 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 7.6514e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0298 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0394 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0012 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0301 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 9.0705e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0332 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 8.4311e-04 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0352 - val_sparse_categorical_accuracy: 0.9849\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 9.1107e-04 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0536 - val_sparse_categorical_accuracy: 0.9849\n",
      "calculate preds....\n",
      "AP score for class 1 ---> 0.9402067113350596\n",
      "f1-score...... 0.7894736842105263\n",
      "recall...... 0.6818181818181818\n",
      "recall...... 0.9375\n",
      "------------------------------------------------------\n",
      "--------- Fold  10 -------------------------\n",
      "Length train index.... 4644\n",
      "Length test index.... 516\n",
      "postive samples in train...... 183\n",
      "postive samples in validation...... 20\n",
      "postive samples in test...... 22\n",
      "------------------------------------------\n",
      "cleaned train docs (in tokens) loaded....\n",
      "train max.... 77\n",
      "cleaned validation docs (in tokens) loaded....\n",
      "train max.... 33\n",
      "cleaned test docs (in tokens) loaded....\n",
      "test max.... 34\n",
      "Model: \"cnn-glove-classifier-harvey\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_28 (InputLayer)       [(None, 128)]             0         \n",
      "                                                                 \n",
      " model_13 (Functional)       (None, 17968)             3214968   \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 17968)             0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                1150016   \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,365,114\n",
      "Trainable params: 4,365,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 7s 175ms/step - loss: 0.1939 - sparse_categorical_accuracy: 0.9450 - val_loss: 0.1146 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0803 - sparse_categorical_accuracy: 0.9562 - val_loss: 0.0575 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0445 - sparse_categorical_accuracy: 0.9722 - val_loss: 0.0501 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 6s 167ms/step - loss: 0.0357 - sparse_categorical_accuracy: 0.9883 - val_loss: 0.0498 - val_sparse_categorical_accuracy: 0.9720\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 5s 167ms/step - loss: 0.0327 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.0446 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0311 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.0448 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0304 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0501 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0294 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0607 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0289 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0531 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0283 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0505 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0278 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0620 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0539 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0268 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0531 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0264 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0572 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0259 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0497 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0255 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0481 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0251 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0584 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0247 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0449 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0545 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0238 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0529 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 6s 167ms/step - loss: 0.0234 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0453 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0446 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 6s 167ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0484 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0221 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0494 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0437 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0464 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0210 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0491 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0476 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0202 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0486 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0199 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0425 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0491 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0504 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0533 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0459 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0487 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0179 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0437 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0175 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0502 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0444 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0169 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0477 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0166 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0423 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0482 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0413 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0470 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.0155 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0449 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0152 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0441 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0086 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0347 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.0064 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0431 - val_sparse_categorical_accuracy: 0.9914\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0036 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0359 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0456 - val_sparse_categorical_accuracy: 0.9871\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.0017 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0593 - val_sparse_categorical_accuracy: 0.9849\n",
      "calculate preds....\n",
      "AP score for class 1 ---> 0.9001743780516158\n",
      "f1-score...... 0.7428571428571429\n",
      "recall...... 0.5909090909090909\n",
      "recall...... 1.0\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rep_fold = 1\n",
    "padd_len = 128\n",
    "ls_save_results = []\n",
    "\n",
    "pos_proba_ls = []\n",
    "y_tests_ls = []\n",
    "preds_ls = []\n",
    "\n",
    "################################################\n",
    "######### Classifier parameters ################\n",
    "################################################\n",
    "learning_rate = 0.0005\n",
    "batch_size = 128\n",
    "#hidden_units = 256\n",
    "num_epochs = 50\n",
    "dropout_rate = 0.3\n",
    "num_classes = 2\n",
    "################################################\n",
    "################################################\n",
    "################################################\n",
    "\n",
    "for train_index, test_index in skf.split(TX,TY):\n",
    "    print('--------- Fold ',str(rep_fold),'-------------------------')\n",
    "    print('Length train index....',len(train_index))\n",
    "    print('Length test index....',len(test_index))\n",
    "    \n",
    "    #### training/testing data ########################################\n",
    "    X_train, X_test = TX[train_index], TX[test_index]\n",
    "    y_train, y_test = TY[train_index], TY[test_index]\n",
    "    \n",
    "    Train_X_cv, val_X, Train_Y_cv, val_Y = train_test_split(X_train,y_train, \n",
    "                                                            random_state=2018, \n",
    "                                                            test_size=0.1,\n",
    "                                                            stratify=y_train)\n",
    "        \n",
    "    ####### check distribution of positive samples on each fold ######\n",
    "    count_test = (y_test == 1).sum()\n",
    "    count_train = (Train_Y_cv == 1).sum()\n",
    "    count_val = (val_Y == 1).sum()\n",
    "    print('postive samples in train......',count_train)\n",
    "    print('postive samples in validation......',count_val)\n",
    "    print('postive samples in test......',count_test)\n",
    "    print('------------------------------------------')\n",
    "    ###################################################################\n",
    "    \n",
    "    train_documents = list()\n",
    "    for row in Train_X_cv: \n",
    "        tweet =  row   #row[\"text\"]\n",
    "        tokens = clean_document_vocab(tweet, vocab)\n",
    "        train_documents.append(tokens)\n",
    "    print('cleaned train docs (in tokens) loaded....')\n",
    "    max_length1 = max([len(s) for s in train_documents])\n",
    "    print('train max....',max_length1)\n",
    "    \n",
    "    validation_documents = list()\n",
    "    for row in val_X: \n",
    "        tweet =  row   #row[\"text\"]\n",
    "        tokens = clean_document_vocab(tweet, vocab)\n",
    "        validation_documents.append(tokens)\n",
    "    print('cleaned validation docs (in tokens) loaded....')\n",
    "    max_length1 = max([len(s) for s in validation_documents])\n",
    "    print('train max....',max_length1)\n",
    "    \n",
    "    test_documents = list()\n",
    "    for row in X_test: \n",
    "        tweet =  row   #row[\"text\"]\n",
    "        tokens = clean_document_vocab(tweet, vocab)\n",
    "        test_documents.append(tokens)\n",
    "    print('cleaned test docs (in tokens) loaded....')\n",
    "    max_length1 = max([len(s) for s in test_documents])\n",
    "    print('test max....',max_length1)\n",
    "    \n",
    "    # covert input text to sequence of indices\n",
    "    train_encoded_docs = tokenizer.texts_to_sequences(train_documents)\n",
    "    val_encoded_docs = tokenizer.texts_to_sequences(validation_documents)\n",
    "    test_encoded_docs = tokenizer.texts_to_sequences(test_documents)\n",
    "\n",
    "    Xtrain = pad_sequences(train_encoded_docs, maxlen=padd_len, padding='post')\n",
    "    Xval = pad_sequences(val_encoded_docs, maxlen=padd_len, padding='post')\n",
    "    Xtest = pad_sequences(test_encoded_docs, maxlen=padd_len, padding='post')\n",
    "\n",
    "    train_labels = Train_Y_cv.tolist()\n",
    "    dev_labels = val_Y.tolist()\n",
    "    test_labels = y_test.tolist()\n",
    "\n",
    "    ###################################################################\n",
    "    ############## Create CNN model in Keras ##########################\n",
    "    ###################################################################\n",
    "    embed_params={'EMBEDDING_DIM':EMBEDDING_DIM,'Tokenizer_size':len(tokenizer.word_index),'weights':embedding_matrix,'input_length':128}\n",
    "    encoder = create_kim_encoder(128, vocab_size,True,embed_params)\n",
    "    classifier = create_classifier(encoder)\n",
    "    classifier.summary()\n",
    "    \n",
    "    ###################################################################\n",
    "    ############## Train CNN model  ###################################\n",
    "    ###################################################################\n",
    "    training_padded = np.array(Xtrain)\n",
    "    training_labels = np.array(train_labels)\n",
    "\n",
    "    validation_padded = np.array(Xval)\n",
    "    validation_labels = np.array(dev_labels)\n",
    "    \n",
    "    testing_padded = np.array(Xtest)\n",
    "    testing_labels = np.array(test_labels)\n",
    "\n",
    "    # compile classifier\n",
    "    classifier.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    # train classifier\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15,restore_best_weights=True)\n",
    "    history = classifier.fit(x=training_padded,validation_data= (validation_padded,validation_labels), y=training_labels, batch_size=batch_size, epochs=num_epochs,callbacks=[callback])\n",
    "    \n",
    "    ###### Save model ####################################\n",
    "    model_name = 'cnn_ian_ida_fold'+ str(rep_fold) + '.h5'\n",
    "    classifier.save(model_name)\n",
    "    ######################################################\n",
    "    \n",
    "    ###################################################################\n",
    "    ############## Predict using CNN model  ###########################\n",
    "    ###################################################################\n",
    "    print('calculate preds....')\n",
    "    predictions = classifier.predict(testing_padded)\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    dict_r = classification_report(y_test.tolist(), preds, output_dict = True)\n",
    "\n",
    "    ### calculate probs for precision-recall curve calculation ######\n",
    "    pos_probs = predictions[:, 1]\n",
    "    pos_proba_ls.append(pos_probs)\n",
    "    y_tests_ls.append(y_test.tolist())\n",
    "    preds_ls.append(preds)\n",
    "    #################################################################\n",
    "        \n",
    "    \n",
    "    ###### calculate precision-recall curve and metric #########\n",
    "    _fold_AP = average_precision_score(y_test.tolist(),pos_probs)\n",
    "    print(\"AP score for class 1 --->\",_fold_AP)\n",
    "    ############################################################\n",
    "    \n",
    "    \n",
    "    ### calculate results ####################################\n",
    "    _f1 = dict_r['1']['f1-score']\n",
    "    _recall = dict_r['1']['recall']\n",
    "    _precision = dict_r['1']['precision']\n",
    "    ##########################################################\n",
    "\n",
    "    print('f1-score......',_f1)\n",
    "    print('recall......',_recall)\n",
    "    print('recall......',_precision)\n",
    "    \n",
    "    fold_results = {'report':dict_r,'f1':_f1,'recall':_recall,'precision':_precision,'AP':_fold_AP,'AP_list':pos_probs,'ytest':y_test.tolist()}\n",
    "    ls_save_results.append(fold_results)\n",
    "\n",
    "\n",
    "    rep_fold = rep_fold + 1\n",
    "    print('------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores =[]\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "AP_scores =[]\n",
    "\n",
    "for i in ls_save_results:\n",
    "    AP_scores.append(i['AP'])\n",
    "    f1_scores.append(i['report']['1']['f1-score'])\n",
    "    recall_scores.append(i['report']['1']['recall'])\n",
    "    precision_scores.append(i['report']['1']['precision'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
